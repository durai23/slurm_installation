# goal
write playbook such that will be able to add instances later and then re-run.
re-run should not affect older instances on which tasks were already run.
it should make changes only on newly created instances.

# brief overview of steps
install mariadb on master node
create users on master and compute nodes
install munge on master and compute nodes
generate key on master node
copy keys to all compute nodes
install slurm dependencies on master and compute nodes
download slurm rpm on master nodes
copy rpm files to all compute nodes
install slurm on all nodes
configure slurm.conf on master node
cppy slurm.conf to all compute nodes
create and change ownership of spool and log folders on master node
create and change ownership of spool and log folders on all compute nodes

SLURM.CONF sources

slothparadise
ControlMachine: buhpc3
ControlAddr: 128.197.116.18
NodeName: buhpc[1-6]
CPUs: 4
StateSaveLocation: /var/spool/slurmctld
SlurmctldLogFile: /var/log/slurmctld.log
SlurmdLogFile: /var/log/slurmd.log
ClusterName: buhpc

additionally, from figure


STOP ANSIBLE DO MANUAL
at the slurm.conf stage I went manual. These are the steps I performed manually
slurm.conf editing and copying to all nodes
slurmdbd.conf editing (set default password as some_pass)
mariadb setup (set default password as some_pass)
	sudo systemctl enable/start mariadb
	run sudo mysql_secure_installation to set up root password for mysql
	sudo mysql -p and the ensuing commands from manual(s)

RETURN TO ANSIBLE
mkdir,touch adn chown commands - btw these are identical in slothparadise and the thai manual and even in yet another manual!

this was the error when attempting to start slumctld 
Nov 25 18:29:25 da-slurm0.novalocal systemd[1]: Starting Slurm controller daemon...
Nov 25 18:29:25 da-slurm0.novalocal systemd[1]: Can't open PID file /var/run/slurmctld.pid (yet?) after start: No such file or directory

checked and found no file called slurmctld.pid in /var/run. so decided to sudo touch create one manually

above did not fix it. found in systemctl status slurmdbd:
Nov 25 15:32:16 da-slurm0.novalocal slurmdbd[10584]: (null): _log_init: Unable to open logfile `/var/log/slurm/slurmdbd.log': No such file or directory

so now will touch /var/log/slurmdbd.log and then make the same entry in slurmdbd.conf and restart slurmdbd

another thing to note is that the "chown slurm: /...." command turns both user and group to slurm but when you do it in ansible by setting owner ONLY then user turns to slurm but group remains root

changing accountingstoragehost in slurm.conf to default just =<blank> on da_slurm0

above fixed the problem
need to copy this new slurm.conf to da_slurm1

services that need to be active before slurmctld on master node
 mariadb
 munge
 slurmdbd

NodeAddr has to be added to the slurm.conf if the hosts are not specified in the /etc/hosts file. if they are listed in /etc/hosts then no need to specify NodeAddr in slurm.conf

/etc/passwd has  the users
/etc/group has the groups

/etc/init.d/<service> can be used to start services
For example 
/etc/init.d/munge start

Ben says DO NOT write stuff into /etc/ instead create folder /home/centos/ansible-slurm and do all your stuff there for example /home/centos/ansible-slurm/munge/munge.key

MUNGE ERROR only on compute:
says keyfile should not be readable or writeable for group or world. but no errors on master (even though keyfile is readable on master to group and world)

systemctl commands to enable need to be sudo


Source pages and notes:

[1] https://www.slothparadise.com/how-to-install-slurm-on-centos-7-cluster/
[2] https://twiki.cern.ch/twiki/bin/view/Main/InstallingCentOS7LocalSite#Slurm_Installation
[3] http://wiki.sc3.uis.edu.co/index.php/Slurm_Installation_on_Debian
[4] https://innuendo.readthedocs.io/en/latest/installation/slurm.html
[5] https://medium.com/@bankz.sukonvijit/%E0%B8%95%E0%B8%B4%E0%B8%94%E0%B8%95%E0%B8%B1%E0%B9%89%E0%B8%87-slurm-%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B9%80%E0%B8%94%E0%B9%87%E0%B8%81%E0%B8%88%E0%B8%9A%E0%B9%83%E0%B8%AB%E0%B8%A1%E0%B9%88-ce4dd2885a55
[6] https://www.strike3d.it/index.php/en/2018/11/06/slurm-on-centos-6/
[7] https://jetatar.github.io/ElasticCloud.html



1 and 2 are basically the same except they use different node names.
systemctl command has enable/disable/status/start/stop options
3 - compare the database setup with volker's guide.
3 uses init.d way instead of systemctl start
4 has some init.d way. Also has slurm.conf with different parameters for learning purpose.
5 has the location of the slurmdbd.conf.example file
6 
7 - compare the database setup with volker's guide. Also firewall and clock.


1) If yo compare 1 and 2 you will get an idea how to fill slurm.conf. Because these two sources use different cluster names so that will be different in the slurm.conf that they present.